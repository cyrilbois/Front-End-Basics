# 了解 GPT

## 前置知识

### NLP（Natural Language Processing）

NLP 全称自然语言处理技术，它是一种人工智能技术，旨在解决计算机与人类语言之间的交互问题。

NLP 技术的目标是让计算机能够理解、分析、处理、生成和对话自然语言。

#### NLP 的难点

让机器理解人类语言的难点在于：

- 同一个意思可以有多种不同的表达形式，可以用一个词，也可以用一段描述
- 同一个表达在不同语境中又有不同含义

### 深度学习

深度学习是一种机器学习算法，其基于人工神经网络和深度神经网络的思想，可以有效地解决图像和语音识别、自然语言处理、机器翻译和预测等任务。

### GPT（Generative Pre-trained Transformer）

GPT 全称：生成式预训练 Transformer

#### Generative（生成式）

GPT 是一种单向的语言模型，也叫自回归模型，即通过前面的文本预测后面的词，训练时以预测能力为主，只根据前文的信息生成后文。

延伸：还有一种语言模型代表 Bert，是双向语言模型，进行文本预测时会结合前文后文信息，以“完形填空”的方式进行文本预测。

#### Pre-trained（预训练）

预训练让模型学习到一些通用的特征和知识

#### Transformer

Transformer 是一种基于编码器和解码器结构的神经网络模型。

最初由 Google 在 2017 年提出，用于自然语言处理（NLP）领域。Transformer 是一种基于自注意力机制（self-attention mechanism）的模型，可以在输入序列中进行全局信息的交互和计算，从而获得比传统循环神经网络更好的长距离依赖建模能力。

## GPT 发展

- GPT1：2018 年发布，1.17 亿参数，5GB 训练数据
  - 架构：采用了 Transfomer 架构中 decoder 部分，无监督学习以及有监督训练，下游可针对不同任务进行微调
- GPT2：2019 年发布，15 亿参数，40GB 训练数据，
- 架构：用了 Transfomer 架构中 decoder 部分，强化无监督学习能力，提出了 FewShot 少样本学习思想，希望模型自己能够理解输入的任务并执行
- GPT3：2020 年发布，沿用 GPT 2 的架构，区别在于超大规模训练量（钞能力），1750 亿参数，45TB 训练数据
- GPT3.5（ChatGPT）：基于 GPT3 同样的参数和训练数据，引入更多的对话，代码训练数据，引入强化学习机制和奖励模型机制，使得模型有能力根据人类的提示生成符合人类偏好的文本

### 零样本，单样本，少样本学习

传统语言模型中，通常的做法是在一个预训练模型通过微调来适应不同的下游任务。在 GPT 中提出了「少样本学习（Few-Shot）」概念。

少样本学习的工作方式是给出 K 个上下文和完成结果的示例，然后给出一个上下文的最终示例，模型需要提供完成结果。单样本只给出一个示例，模型即可以完成任务。而零样本则是不需要给出示例，直接让语言模型执行相应任务。

零样本，单样本，少样本之间区别，区别在于给出示例的多少。零样本是不给出示例直接让模型执行任务

## 底层原理

ChatGPT 可以类比为学说话的鹦鹉，它没有意识、没有欲望、没有情绪，甚至都不理解自己说了什么。

它的实质功能（原理）是“单字接龙”。给他任意长度的**上文**，它会用自己的**模型**生成下一个字。然后再用这个字跟之前的上文组成**新的上文**，再生成下一个字，不断重复，就可以生成任意长的下文了，这种方式称为自回归生成。这就是 ChatGPT 回答问题的方式。

从上面的描述可以看出，ChatGPT 的核心是**上文**和**模型**，模型的核心是**参数**。模型的参数是通过**训练**得到的，训练的核心是**数据**。

通俗解释：ChatGPT 从根本上始终要做的是，针对它得到的任何文本产生“合理的延续”。这里所说的“合理”是指，“人们在看到诸如数十亿个网页上的内容后，可能期待别人会这样写”。

### GPT 的训练过程概述

- 收集数据：收集大量的文本数据（GPT3 是 45TB），例如维基百科、小说、新闻文章等。
- 分词和编码：对文本数据进行分词和编码，将其转换成数字形式，以便于计算机进行处理。
- 预训练：使用 Transfomer 架构中 decoder 部分对分词后的数据进行预训练，预训练的目标是让 GPT 能够根据给定的上文预测出下一个单词。在这个过程中，GPT 能够**学习**到语言的结构和规律，从而提高其语言的理解和生成能力。
  - 训练的目的是学习，而不是记忆。是为了应对没有记忆过的内容（记忆过的内容可以理解为数据库已经存储过的内容）时使用学习到的通用规律生成合理的内容。这种举一反三的目的也叫做泛化。
- 微调：对预训练好的 GPT 进行微调，使其能够更好地适应特定任务。例如：在对话生成任务中，使用对话数据集对 GPT 进行微调，能使其更高的生成自然流畅的对话内容
- 验证和评估：使用验证集和评估指标来测试 GPT 的性能，如果性能不足，可以重新调整模型参数或者增加更多的训练数据，以提高模型性能。

GPT 的训练过程就是一个迭代的过程，需要不断地收集数据、预处理数据、预训练模型、微调模型、并对模型进行验证和评估，以不断提高模型性能。

通俗解释：ChatGPT（或者说它基于的 GPT-3 网络）到底是在做什么呢？它的总体目标是，根据所接受的训练（查看来自互联网的数十亿页文本，等等），以“合理”的方式续写文本。所以在任意给定时刻，它都有一定量的文本，而目标是为要添加的下一个标记做出适当的选择。

## 预测的实质

GPT 根据上文预测下一个词时会列出随后可能出现的词及其出现的“概率”（按“概率”从高到低排列），比如写一篇文章时，它实质上只是在一遍又一遍地询问“根据目前的文本，下一个词应该是什么”，并且每次都添加一个词。

每一步都会得到一个带概率的词列表。但它应该选择将哪一个词添加到正在写作的文章中呢？有人可能认为应该选择“排名最高”的词，即分配了最高“概率”的词。然而，这里出现了一点儿玄学的意味。如果我们总是选择排名最高的词，通常会得到一篇非常“平淡”的文章，完全显示不出任何“创造力”（有时甚至会一字不差地重复前文，为什么会这样暂时没有科学解释）。但是，如果有时（随机）选择排名较低的词，就会得到一篇“更有趣”的文章。

这里存在随机性意味着，如果我们多次使用相同的提示(prompt)，每次都有可能得到不同的文章。而且，符合玄学思想的是，有一个所谓的“温度”（Temperature）参数来确定低排名词的使用频率。

## chatGPT 的三步训练

### 第一步-预训练-无监督学习（开卷有益）

#### NLP 的难点

让机器理解人类语言的难点在于：

- 同一个意思可以有多种不同的表达形式，可以用一个词，也可以用一段描述
- 同一个表达在不同语境中又有不同含义

想解决这个问题，需要让机器学会各种“语义关系”和“语法规律”，以便能明白“哪些表达实际上是同一个意思”

对此，GPT 的办法是：让模型看到尽可能多、尽可能丰富的「语言范例（学习材料）」，使其有更多机会建构出能举一反三的语言规律，来应对无数「从未见过的语言」。这一阶段称为“开卷有益”。GPT 中的“G”代表“生成（Generative）”，“T”代表“Transformer”一种模型结构，而“P”（Pre-training）代表的就是“开卷有益”这一步，专业名称叫“预训练”，也成为无监督学习

#### 预训练产生的问题

“开卷有益”却存在一个问题：尽管 GPT 拥有了海量的知识，但回答形式和内容却不受约束。因为它知道的太多了，见到了一个人几辈子都读不完的资料，会随意联想，它有能力回答我们的问题，但我们却很难指挥它。

要如何解决这个问题，使 GPT 能够跟我们合理的对话？

### 第二步-有监督学习（模版规范）

用「对话模板」去矫正它在“开卷有益”时所学到的「不规范“习惯”」。具体做法是：不再用随便的互联网文本，而把人工专门写好的「优质对话范例」给「“开卷有益”后的 GPT-3」，让它再去做单字接龙，从而学习「如何组织符合人类规范的回答」。这一阶段称为“模板规范”。

例如：对没联网的 GPT，问最新新闻，要回答不知道；用户的提问有错误时，应该指出错误，而不是顺着瞎编；不回答有害内容，也就是什么该说什么不该说等。

#### 涌现的能力

- “理解”指令要求的能力：指「能按照用户的抽象描述，给出处理结果」，例如“翻译”这个指令，GPT 就已经掌握了。
- “理解”例子要求的能力：指「能按照用户给的若干具体例子，来处理新内容」，意味着，如果你以后不明白怎么给它描述指令，就可以「通过给它举几个例子，来让它明确你想干什么」。这项能力十分神奇，因为看起来 ChatGPT 仿佛掌握了「如何通过例子来学习」的能力，而这个能力又是我们通过模板范文（例子）让它学会的。产生了一种“它学会了如何学习”的套娃感。大家把这种现象称为“语境内学习（In-context Learning）”，目前对这种能力的产生原因还没有定论。

#### 分治效应

在超大模型的使用中，大家还发现了一种「分治效应」：当 ChatGPT 无法答对一个综合问题时，若要求它分步思考，它就可以一步步连续推理，且最终答对的可能性大幅提升，该能力也叫“思维链”。

ChatGPT 的思维链能力，可能是在训练做代码的单字接龙后所产生的。因为人类在面对复杂任务时，直接思考答案也会没头绪，用分而治之往往可以解决。因此大家猜测，ChatGPT 可能是通过对代码的单字接龙，学到了代码中所蕴含的「人类分治思想」。不过目前对该现象的产生原因也没有定论。

#### 规模效应

可以切实地感受到，单字接龙的结构虽然简单，但被扩展到超大规模后，所能展现出的能力有多出乎意料。

在小单字接龙模型中，并没有察觉出「“理解”指令」「“理解”例子」「思维链」的能力，但在超大模型中，却突然展现。因此人们也用“涌现”这个词来描述「这些能力的出现」。

### 第三步-强化学习（创意引导）

经过“开卷有益”和“模版规范”这两个训练阶段后，超大单字接龙模型已经变得极其强大了。但“模板规范”的训练阶段也存在不足，那就是：可能导致 ChatGPT 的回答过于模板化，限制其创造力。

不再要求它按照我们提供的对话范例做单字接龙，而是直接向它提问，再让它自由回答。如果回答得妙，就给予奖励，如果回答不佳，就降低奖励。然后利用这些「人类评分」去调整 ChatGPT 的模型。

在这种训练中，既不会用现有的模板来限制它的表现，又可以引导它创造出符合人类认可的回答。这一阶段称为“创意引导”。

总结一下，本章讲了 ChatGPT 的三个训练阶段：

- “开卷有益”阶段：让 ChatGPT 对「海量互联网文本」做单字接龙，以扩充模型的词汇量、语言知识、世界的信息与知识。使 ChatGPT 从“哑巴鹦鹉”变成“脑容量超级大的懂王鹦鹉”。

- “模板规范”阶段：让 ChatGPT 对「优质对话范例」做单字接龙，以规范回答的对话模式和对话内容。使 ChatGPT 变成“懂规矩的博学鹦鹉”。

- “创意引导”阶段：让 ChatGPT 根据「人类对它生成答案的好坏评分」来调节模型，以引导它生成人类认可的创意回答。使 ChatGPT 变成“既懂规矩又会试探的博学鹦鹉”。

## ChatGPT 的缺点

### 胡编混淆

为了能应对「未被记忆的情况」，它会学习语言单位（如单词、短语、句子等）之间的规律，用「学到的规律」来成回答，然而，这也意味着如果出现了「实际不同但碰巧符合同一个规律」的内容，模型就可能混淆它。

最直接的结果是：若「现实中不存在内容」刚好符合「它从训练材料中学到的规律」，那 ChatGPT 就有可能对「不存在内容」进行「合乎规律的混合捏造」。

### 无法直接增删改查

不论是 ChatGPT 「所记住的信息」，还是「所学到的规律」，都是以同一个模型的形式来表达的，因此我们无法像操作数据库那样，对这些内容直接进行增删改查。

这会导致两个具体问题：

第一：由于我们很难理解它所建构的规律，又无法直接查看它记住了什么、学到了什么，只能通过多次提问来评估和猜测它的所记所学，其决策缺乏可解释性，这难免会在使用时带来安全风险。

第二：由于只能通过再次调整模型（即再次训练）来增加、删除或修改它的所记所学，这难免在更新时会降低效率。

## 未来影响

## 如何应对

## 不同人、职业、角度对 GPT 的理解

- 数据产品经理：[大白话讲清楚 ChatGPT](https://mp.weixin.qq.com/s/tri0OINMaOxYeqSk8iFesg)
- 研发：[初探 chatgpt](https://mp.weixin.qq.com/s/nUgoxyLpYOdUzCzyjrs7xw)
- [详解｜程序员如何适应 AI 时代的新需求](https://mp.weixin.qq.com/s/_qKk7KDsAGbR-jfvwlZULQ)

## 其他相关

## 国内使用 AI

- [在中国国内如何购买 ChatGPT Plus](https://www.digitalnomadlc.com/how-to-buy-chatgptplus/)

## 文章

- [ChatGPT 原理探索](https://juejin.cn/post/7218048201982787645)
- [大白话讲清楚 ChatGPT](https://mp.weixin.qq.com/s/tri0OINMaOxYeqSk8iFesg)
- [初探 chatgpt](https://mp.weixin.qq.com/s/nUgoxyLpYOdUzCzyjrs7xw)
